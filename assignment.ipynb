{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "source": [
    "# TM10007 Assignment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "CiDn2Sk-VWqE",
    "outputId": "64224cd2-6054-4b04-a3f6-af8290400dfc"
   },
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NE_fTbKGe5z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples: 115\n",
      "The number of columns: 494\n",
      "The number of liposarcoma in the dataset: 58\n",
      "The number of lipoma in the dataset: 57\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions. Uncomment the one you want to use\n",
    "# Import other classifiers you plan to use\n",
    "from worclipo.load_data import load_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "data = load_data()\n",
    "print(f\"The number of samples: {len(data.index)}\")\n",
    "print(f\"The number of columns: {len(data.columns)}\")\n",
    "print(f\"The number of liposarcoma in the dataset: {len(data[data['label'] == 'liposarcoma'])}\")\n",
    "print(f\"The number of lipoma in the dataset: {len(data[data['label'] == 'lipoma'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know the number of samples, colums and labels. The next step is to look for missing data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (92, 493)\n",
      "Testing set size: (23, 493)\n"
     ]
    }
   ],
   "source": [
    "def split_data(data):\n",
    "\n",
    "    # Separate features and target variable\n",
    "    x = data.drop(['label'], axis=1)\n",
    "    y = data['label']\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Call the function\n",
    "x_train, x_test, y_train, y_test = split_data(data)\n",
    "\n",
    "# Optionally, print the sizes of the splits to verify\n",
    "print(\"Training set size:\", x_train.shape)\n",
    "print(\"Testing set size:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aantal 0's per feature:\n",
      " label                                                       0\n",
      "PREDICT_original_sf_compactness_avg_2.5D                    0\n",
      "PREDICT_original_sf_compactness_std_2.5D                    0\n",
      "PREDICT_original_sf_rad_dist_avg_2.5D                       0\n",
      "PREDICT_original_sf_rad_dist_std_2.5D                       0\n",
      "                                                         ... \n",
      "PREDICT_original_phasef_phasesym_peak_position_WL3_N5     115\n",
      "PREDICT_original_phasef_phasesym_range_WL3_N5               0\n",
      "PREDICT_original_phasef_phasesym_energy_WL3_N5              0\n",
      "PREDICT_original_phasef_phasesym_quartile_range_WL3_N5     12\n",
      "PREDICT_original_phasef_phasesym_entropy_WL3_N5             0\n",
      "Length: 494, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def find_zeros_in_data(data):\n",
    "    # Maak een kopie van de data om de originele data niet te wijzigen\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    # Vervang waarden die gelijk zijn aan 0 met NaN om ze gemakkelijk te tellen\n",
    "    data_copy = data_copy.replace(0, np.nan)\n",
    "    \n",
    "    # Tel het aantal NaNs in elke kolom, wat overeenkomt met het originele aantal 0's\n",
    "    zeros_count = data_copy.isna().sum()\n",
    "    \n",
    "    # Print het aantal 0's gevonden in elke kolom\n",
    "    print(\"Aantal 0's per feature:\\n\", zeros_count)\n",
    "    \n",
    "    # Return het aantal 0's per feature voor verdere analyse indien nodig\n",
    "    return zeros_count\n",
    "\n",
    "# Roep de functie aan met je dataset 'data'\n",
    "zeros_count = find_zeros_in_data(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_data(x_train,x_test):\n",
    "    scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "\n",
    "    # Applying scaler to train and test set\n",
    "    X_train_scaled = scaler.transform(x_train)\n",
    "    X_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m:  \u001b[38;5;66;03m# Exclude the label column\u001b[39;00m\n\u001b[0;32m      8\u001b[0m         t_statistic, p_value \u001b[38;5;241m=\u001b[39m ttest_ind(lipoma[feature], liposarcoma[feature])\n\u001b[1;32m----> 9\u001b[0m         feature_result \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m: feature, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt_statistic\u001b[39m\u001b[38;5;124m'\u001b[39m: t_statistic, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp_value\u001b[39m\u001b[38;5;124m'\u001b[39m: p_value})\n\u001b[0;32m     10\u001b[0m         Ttest_indResult \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([Ttest_indResult, feature_result])\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(Ttest_indResult)\n",
      "File \u001b[1;32mc:\\Users\\31642\\miniconda3\\Lib\\site-packages\\pandas\\core\\frame.py:733\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    727\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    728\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    729\u001b[0m     )\n\u001b[0;32m    731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(data, index, columns, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy, typ\u001b[38;5;241m=\u001b[39mmanager)\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\31642\\miniconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_to_mgr(arrays, columns, index, dtype\u001b[38;5;241m=\u001b[39mdtype, typ\u001b[38;5;241m=\u001b[39mtyp, consolidate\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[1;32mc:\\Users\\31642\\miniconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m _extract_index(arrays)\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\31642\\miniconda3\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:667\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    664\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    666\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n\u001b[1;32m--> 667\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf using all scalar values, you must pass an index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m union_indexes(indexes)\n",
      "\u001b[1;31mValueError\u001b[0m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "#define samples\n",
    "lipoma = data[data['label']=='lipoma']\n",
    "liposarcoma = data[data['label']=='liposarcoma']\n",
    "\n",
    "Ttest_indResult = pd.DataFrame(columns=['Feature', 't_statistic', 'p_value'])\n",
    "for feature in data.columns:\n",
    "    if feature != 'label':  # Exclude the label column\n",
    "        t_statistic, p_value = ttest_ind(lipoma[feature], liposarcoma[feature])\n",
    "        feature_result = pd.DataFrame({'Feature': feature, 't_statistic': t_statistic, 'p_value': p_value})\n",
    "        Ttest_indResult = pd.concat([Ttest_indResult, feature_result])\n",
    "\n",
    "print(Ttest_indResult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
