{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "source": [
    "# TM10007 Assignment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "CiDn2Sk-VWqE",
    "outputId": "64224cd2-6054-4b04-a3f6-af8290400dfc"
   },
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NE_fTbKGe5z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples: 115\n",
      "The number of columns: 494\n",
      "The number of liposarcoma in the dataset: 58\n",
      "The number of lipoma in the dataset: 57\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions. Uncomment the one you want to use\n",
    "# Import other classifiers you plan to use\n",
    "from worclipo.load_data import load_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "data = load_data()\n",
    "print(f\"The number of samples: {len(data.index)}\")\n",
    "print(f\"The number of columns: {len(data.columns)}\")\n",
    "print(f\"The number of liposarcoma in the dataset: {len(data[data['label'] == 'liposarcoma'])}\")\n",
    "print(f\"The number of lipoma in the dataset: {len(data[data['label'] == 'lipoma'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know the number of samples, colums and labels. The next step is to look for missing data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (92, 493)\n",
      "Testing set size: (23, 493)\n"
     ]
    }
   ],
   "source": [
    "def split_data(data):\n",
    "\n",
    "    # Separate features and target variable\n",
    "    x = data.drop(['label'], axis=1)\n",
    "    y = data['label']\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Call the function\n",
    "x_train, x_test, y_train, y_test = split_data(data)\n",
    "\n",
    "# Optionally, print the sizes of the splits to verify\n",
    "print(\"Training set size:\", x_train.shape)\n",
    "print(\"Testing set size:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aantal 0's per feature:\n",
      " label                                                       0\n",
      "PREDICT_original_sf_compactness_avg_2.5D                    0\n",
      "PREDICT_original_sf_compactness_std_2.5D                    0\n",
      "PREDICT_original_sf_rad_dist_avg_2.5D                       0\n",
      "PREDICT_original_sf_rad_dist_std_2.5D                       0\n",
      "                                                         ... \n",
      "PREDICT_original_phasef_phasesym_peak_position_WL3_N5     115\n",
      "PREDICT_original_phasef_phasesym_range_WL3_N5               0\n",
      "PREDICT_original_phasef_phasesym_energy_WL3_N5              0\n",
      "PREDICT_original_phasef_phasesym_quartile_range_WL3_N5     12\n",
      "PREDICT_original_phasef_phasesym_entropy_WL3_N5             0\n",
      "Length: 494, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def find_zeros_in_data(data):\n",
    "    # Maak een kopie van de data om de originele data niet te wijzigen\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    # Vervang waarden die gelijk zijn aan 0 met NaN om ze gemakkelijk te tellen\n",
    "    data_copy = data_copy.replace(0, np.nan)\n",
    "    \n",
    "    # Tel het aantal NaNs in elke kolom, wat overeenkomt met het originele aantal 0's\n",
    "    zeros_count = data_copy.isna().sum()\n",
    "    \n",
    "    # Print het aantal 0's gevonden in elke kolom\n",
    "    print(\"Aantal 0's per feature:\\n\", zeros_count)\n",
    "    \n",
    "    # Return het aantal 0's per feature voor verdere analyse indien nodig\n",
    "    return zeros_count\n",
    "\n",
    "# Roep de functie aan met je dataset 'data'\n",
    "zeros_count = find_zeros_in_data(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_data(x_train,x_test):\n",
    "    scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "\n",
    "    # Applying scaler to train and test set\n",
    "    X_train_scaled = scaler.transform(x_train)\n",
    "    X_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\31642\\AppData\\Local\\Temp\\ipykernel_12572\\3512054077.py:10: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  Ttest_indResult = pd.concat([Ttest_indResult, feature_result])\n",
      "c:\\Users\\31642\\miniconda3\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "c:\\Users\\31642\\miniconda3\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "c:\\Users\\31642\\miniconda3\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "c:\\Users\\31642\\miniconda3\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "c:\\Users\\31642\\miniconda3\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "c:\\Users\\31642\\miniconda3\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "c:\\Users\\31642\\miniconda3\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n",
      "c:\\Users\\31642\\miniconda3\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:523: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    t_statistic   p_value\n",
      "PREDICT_original_sf_compactness_avg_2.5D               0.690264  0.491443\n",
      "PREDICT_original_sf_compactness_std_2.5D              -3.058812  0.002775\n",
      "PREDICT_original_sf_rad_dist_avg_2.5D                 -2.161416  0.032773\n",
      "PREDICT_original_sf_rad_dist_std_2.5D                 -0.618317  0.537611\n",
      "PREDICT_original_sf_roughness_avg_2.5D                 0.632823  0.528128\n",
      "...                                                         ...       ...\n",
      "PREDICT_original_phasef_phasesym_peak_position_...          NaN       NaN\n",
      "PREDICT_original_phasef_phasesym_range_WL3_N5          2.660891  0.008928\n",
      "PREDICT_original_phasef_phasesym_energy_WL3_N5        -2.779793  0.006373\n",
      "PREDICT_original_phasef_phasesym_quartile_range...     3.273034  0.001412\n",
      "PREDICT_original_phasef_phasesym_entropy_WL3_N5       -2.791192  0.006167\n",
      "\n",
      "[493 rows x 2 columns]\n",
      "PREDICT_original_tf_Gabor_mean_F0.2_A0.0        4.360695\n",
      "PREDICT_original_tf_Gabor_mean_F0.2_A0.79       4.045337\n",
      "PREDICT_original_tf_Gabor_mean_F0.5_A0.0        3.707360\n",
      "PREDICT_original_tf_Gabor_median_F0.2_A1.57     3.392270\n",
      "PREDICT_original_phasef_phasesym_mean_WL3_N5    3.325470\n",
      "Name: t_statistic, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#define samples\n",
    "lipoma = data[data['label']=='lipoma']\n",
    "liposarcoma = data[data['label']=='liposarcoma']\n",
    "\n",
    "Ttest_indResult = pd.DataFrame(columns=['t_statistic', 'p_value'])\n",
    "for feature in data.columns:\n",
    "    if feature != 'label':  # Exclude the label column\n",
    "        t_statistic, p_value = ttest_ind(lipoma[feature], liposarcoma[feature])\n",
    "        feature_result = pd.DataFrame({'t_statistic': t_statistic, 'p_value': p_value}, index = [feature])\n",
    "        Ttest_indResult = pd.concat([Ttest_indResult, feature_result])\n",
    "\n",
    "\n",
    "absolute_features = Ttest_indResult['t_statistic'].abs()\n",
    "best_festures = absolute_features.nlargest(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
