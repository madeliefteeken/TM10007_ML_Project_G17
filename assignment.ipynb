{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SXpaKwwGe5x"
   },
   "source": [
    "# TM10007 Assignment template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "CiDn2Sk-VWqE",
    "outputId": "64224cd2-6054-4b04-a3f6-af8290400dfc"
   },
   "outputs": [],
   "source": [
    "# Run this to use from colab environment\n",
    "#!pip install -q --upgrade git+https://github.com/jveenland/tm10007_ml.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and cleaning\n",
    "\n",
    "Below are functions to load the dataset of your choice. After that, it is all up to you to create and evaluate a classification method. Beware, there may be missing values in these datasets. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NE_fTbKGe5z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples: 115\n",
      "The number of columns: 494\n",
      "The number of liposarcoma in the dataset: 58\n",
      "The number of lipoma in the dataset: 57\n"
     ]
    }
   ],
   "source": [
    "# Data loading functions. Uncomment the one you want to use\n",
    "# Import other classifiers you plan to use\n",
    "from worclipo.load_data import load_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, auc\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = load_data()\n",
    "print(f\"The number of samples: {len(data.index)}\")\n",
    "print(f\"The number of columns: {len(data.columns)}\")\n",
    "print(f\"The number of liposarcoma in the dataset: {len(data[data['label'] == 'liposarcoma'])}\")\n",
    "print(f\"The number of lipoma in the dataset: {len(data[data['label'] == 'lipoma'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know the number of samples, colums and labels. The next step is to look for missing data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (92, 493)\n",
      "Testing set size: (23, 493)\n"
     ]
    }
   ],
   "source": [
    "def split_data(data):\n",
    "\n",
    "    # Separate features and target variable\n",
    "    x = data.drop(['label'], axis=1)\n",
    "    y = data['label']\n",
    "    \n",
    "    # Split the dataset into training and testing sets\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Call the function\n",
    "x_train, x_test, y_train, y_test = split_data(data)\n",
    "\n",
    "# Optionally, print the sizes of the splits to verify\n",
    "print(\"Training set size:\", x_train.shape)\n",
    "print(\"Testing set size:\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aantal 0's per feature:\n",
      " label                                                       0\n",
      "PREDICT_original_sf_compactness_avg_2.5D                    0\n",
      "PREDICT_original_sf_compactness_std_2.5D                    0\n",
      "PREDICT_original_sf_rad_dist_avg_2.5D                       0\n",
      "PREDICT_original_sf_rad_dist_std_2.5D                       0\n",
      "                                                         ... \n",
      "PREDICT_original_phasef_phasesym_peak_position_WL3_N5     115\n",
      "PREDICT_original_phasef_phasesym_range_WL3_N5               0\n",
      "PREDICT_original_phasef_phasesym_energy_WL3_N5              0\n",
      "PREDICT_original_phasef_phasesym_quartile_range_WL3_N5     12\n",
      "PREDICT_original_phasef_phasesym_entropy_WL3_N5             0\n",
      "Length: 494, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def find_zeros_in_data(data):\n",
    "    # Maak een kopie van de data om de originele data niet te wijzigen\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    # Vervang waarden die gelijk zijn aan 0 met NaN om ze gemakkelijk te tellen\n",
    "    data_copy = data_copy.replace(0, np.nan)\n",
    "    \n",
    "    # Tel het aantal NaNs in elke kolom, wat overeenkomt met het originele aantal 0's\n",
    "    zeros_count = data_copy.isna().sum()\n",
    "    \n",
    "    # Print het aantal 0's gevonden in elke kolom\n",
    "    print(\"Aantal 0's per feature:\\n\", zeros_count)\n",
    "    \n",
    "    # Return het aantal 0's per feature voor verdere analyse indien nodig\n",
    "    return zeros_count\n",
    "\n",
    "# Roep de functie aan met je dataset 'data'\n",
    "zeros_count = find_zeros_in_data(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_data(x_train,x_test):\n",
    "    scaler = preprocessing.StandardScaler().fit(x_train)\n",
    "\n",
    "    # Applying scaler to train and test set\n",
    "    X_train_scaled = scaler.transform(x_train)\n",
    "    X_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    principal component 1  principal component 2  principal component 3  \\\n",
      "0           -2.096928e+11          -3.489566e+09          -6.398413e+07   \n",
      "1           -1.981416e+11          -2.388704e+09           1.002436e+07   \n",
      "2           -2.101933e+11          -3.574646e+09          -7.010663e+07   \n",
      "3           -2.107059e+11          -3.581685e+09          -7.161583e+07   \n",
      "4           -1.983410e+11          -2.309896e+09           1.925980e+07   \n",
      "..                    ...                    ...                    ...   \n",
      "87          -2.099659e+11          -3.476570e+09          -6.185785e+07   \n",
      "88          -2.040856e+11          -2.818019e+09          -1.175914e+07   \n",
      "89          -1.428941e+11          -3.646570e+09          -7.792137e+07   \n",
      "90          -2.098373e+11          -3.514557e+09          -6.440762e+07   \n",
      "91          -2.100529e+11          -3.537322e+09          -6.821980e+07   \n",
      "\n",
      "    principal component 4  \n",
      "0           -9.903784e+06  \n",
      "1            4.363908e+08  \n",
      "2           -4.424254e+07  \n",
      "3           -4.916480e+07  \n",
      "4            4.845909e+08  \n",
      "..                    ...  \n",
      "87          -2.187647e+06  \n",
      "88           2.718397e+08  \n",
      "89          -8.778187e+07  \n",
      "90          -1.720738e+07  \n",
      "91          -3.127668e+07  \n",
      "\n",
      "[92 rows x 4 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9.99445139e-01, 5.54657147e-04, 1.94820968e-07, 8.85241540e-09])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca = PCA(n_components=4)\n",
    "\n",
    "principalComponents = pca.fit_transform(x_train)\n",
    "\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "             , columns = ['principal component 1', 'principal component 2','principal component 3','principal component 4'])\n",
    "\n",
    "print(principalDf)\n",
    "\n",
    "pca.explained_variance_ratio_"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
